---

# ğŸŒ Recursive Web Crawler in C#

A simple web crawler in C# that recursively explores links from a webpage!

## ğŸ“‹ Features
- ğŸ” **Parse Links**: Starts by parsing all links from a given URL.
- ğŸ”„ **Recursive Crawling**: Visits each parsed link and extracts further links until the maximum limit is reached.
- ğŸ”— **Link Extraction**: Uses regular expressions to extract URLs from the page content.
- ğŸ›¡ **Duplicate Protection**: Maintains a `HashSet` of visited URLs to avoid revisiting and prevent infinite loops.
- ğŸ¯ **Customizable**: Set the maximum number of URLs to crawl.

## ğŸš€ How It Works
1. **Start Crawling**: Specify the starting URL.
2. **Extract Links**: The program fetches the content of the page and extracts all links.
3. **Recursive Visits**: It recursively visits those links, repeating the process.
4. **Stop Condition**: Crawling continues until the defined maximum number of URLs is reached.

---
